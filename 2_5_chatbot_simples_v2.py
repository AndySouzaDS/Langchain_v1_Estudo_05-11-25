from langchain.chat_models import init_chat_model
from dotenv import load_dotenv

load_dotenv()

model = init_chat_model(model="llama-3.3-70b-versatile", model_provider="groq")

nome = input("Seu nome: ")
idade = input("Sua idade: ")

messages = [
    {"role": "system", "content": "VocÃª Ã© um assistente amigÃ¡vel, que personaliza respostas."},
    {"role": "user", "content": f"Meu nome Ã© {nome} e tenho {idade} anos. Me dÃª uma sugestÃ£o de hobby."}
]

response = model.invoke(messages)
print(f"\nğŸ¤– {response.content}")
